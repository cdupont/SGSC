% !TEX root =  main.tex
\section{Related Works}
\label{sec: relworks}
\cd{in a paper of 4 pages, related works should be very short: I'd say half a page.}

Workload consolidation is a powerful means to improve IT efficiency and reduce power consumption.
VM consolidation approaches to reduce energy consumption at IaaS layer have been explored in many recent papers \cite{Cardosa} \cite{ITProf1} \cite{Schroder} \cite{Hermenier2009} \cite{sheikhalishahi_energy_2011} \cite{sheikhalishahi_multi-capacity_2014} \cite{dupont2015plug4green}.

In addition, there are energy efficient solutions based on scaling operations (scale up/down application) based on applications performance metrics.
Although these proposals reduce the energy footprint of applications and cloud infrastructures, they do not model the applications performance trend to finely define a trade-off between applications Quality of Service and energy footprint.

Autonomic Computing has been exploited in the design of cloud computing architectures in order to devise autonomic loops aiming at providing coordinated actions among cloud layers for efficiency measures, turning each layer of the cloud stack more autonomous, adaptable and aware of the runtime environment \cite{alvares_de_oliveira_synchronization_2012} \cite{de_oliveira_self-management_2012}  \cite{de_oliveira_framework_2013}.

In order to reach a global and efficient state due to conflicting objectives, autonomic loops need to be synchronized.
In \cite{alvares_de_oliveira_synchronization_2012}, authors proposed a generic model to synchronize and coordinate autonomic loops in cloud computing stack. 
The feasibility and scalability of their approach is evaluated via simulation-based experiments on the interaction of several self-adaptive applications with a common self-adaptive infrastructure.

In addition to elasticity, scalability is another major advantage introduced by the cloud paradigm.
In \cite{vaquero_dynamically_2011}, automated application scalability in cloud environments are presented.
Authors highlight challenges that will likely be addressed in new research efforts and present an ideal scalable cloud system.

\cite{carpen-amarie_towards_2014} proposes a co-design of IaaS and PaaS layers for energy optimization in the cloud. The paper outlines the design of interfaces to enable cross-layer cooperation in clouds. This position paper claims that significant energy gains could be obtained by creating a cooperation API between the IaaS layer and the PaaS layer. Authors discuss two complementary approaches for establishing such cooperation: cross-layer information sharing, and cross-layer coordination.


%EE

%IaaS EE
%-VM consolidation %-Scheduling
%History of energy optimization in virtualized datacenters, in a single datacenter

%Renewable energy availability can estimate a good measure to provide a good consolidation degree.

%As a recent development in this field, in \cite{sheikhalishahi_multi-capacity_2014}, authors presented a multi-resource scheduling technique to provide a higher degree of consolidation in multi-dimensional computing systems.
% However, there are some important pending issues before making the dreamed automated scaling for applications come true. 
%We present relevant efforts at the edge of state of the art technology, providing an encompassing overview of the trends they each follow. 

%PaaS-IaaS



%In \cite{de_oliveira_self-management_2012}, authors propose a self-adaptation approach that considers both application internals and system to reduce the energy footprint in cloud infrastructure. Each application and the infrastructure are equipped with their own control loop, which allows them to autonomously optimize their executions. Simulations show that the approach may lead to appreciable energy savings without interfering on application provider revenues.
%-Scaling operation, Service Flexibility Agreement


%PaaS EE
%Containers

%move to INTRO

%Many research efforts have been dedicated to reducing cloud energy consumption, in particular by optimizing the Infrastructure-as-a-Service layer of the Cloud. Infrastructure-as-a-Service (IaaS) is the layer in charge of the virtualization of physical resources, and therefore has direct control over energy-related elements. However, the IaaS layer has no knowledge about the nature of applications which run over these resources, which limits the scope of decisions it can take.
%http://www.conpaas.eu/open-postdoc-position/

%The EcoPaaS project therefore aim at making the IaaS layer (in charge of resources) and the PaaS layer (in charge of applications) collaborate to further reduce the Cloud energy consumption. The idea is to define standard interfaces that allow both layers to exchange relevant information and to coordinate their actions. Exchanging information will for example allow the PaaS layer to estimate the energy consumption of each application it is running. Coordinating actions will in turn allow the system to avoid situations where both layers simultaneously take mutually-damaging actions.

%We will therefore investigate difficult scientific questions such as: which information can and should be exchanged between the two layers? What are the necessary coordination mechanisms to control reconfigurations and avoid detrimental side effects between the two layers? And which energy savings can we potentially obtain if we deployed such techniques in large commercial clouds?

%We will address these questions in the context of the ConPaaS system running over an energy-efficient private cloud such as Snooze. We will follow a systems-oriented methodology based on rigorous experimental data collection, formulation of hypotheses about the necessary mechanisms, prototype implementations, and experimental (in-)validation of the hypotheses.

%In point of fact, from the application provider perspective, Autonomic Computing makes application capable of reacting to a highly variable workload by dynamically adjusting the amount of resources needed to be executed while keeping its Quality of Service (QoS) [11]. From the infrastructure provider point of view, it also makes the infrastructure capable of rapidly reacting to environment changes (e.g. increase/decrease of physical resource usage) by optimizing the allocation of resources and thereby reduce the costs related to energy consumption [4].

%Implementing elasticity to tackle varying workloads while optimizing infrastructures (e.g. utilization rate) and fulfilling applications’ requirements on Quality of Service still remains an open issue and should be addressed by self-adaptation techniques able to manage complexity and dynamism. 

%This model is applied to a Cloud Computing scenario in which several self-adaptive applications interact with a common self-adaptive infrastructure. The objective at the application level is to manage the runtime context to minimize costs while maximizing the QoS, whereas at the infrastructure level, the objective is to manage the context to optimize the utilization rate. 

%Each of the two layers contains information which is potentially relevant for the other in order to reduce the energy footprint of a given application. For example, an energy-aware IaaS may have an estimate of the energy footprint of individual virtual machine types, which may be useful for the PaaS to preferentially select energy-efficient VM instance types. Conversely, a PaaS layer may build short-term traffic predictions which constitute useful hints for IaaS-level consolidation algorithms.

%Both layers should coordinate their reconfiguration actions to reach a state where they help each other in achieving their goals rather than potentially take mutually detrimental decisions. For example, if the IaaS layer decides to migrate a specific VM, then the PaaS could in many cases temporarily reduce the load of the concerned VM to facilitate its migration. Conversely, if the PaaS layer gives early warnings to the IaaS about future requests for creating/destroying resources, the IaaS layer can prepare in advance for these changes. 


%Similarly, [4] proposed an hierarchical autonomic approach for multi-tier web applications. The objective is to deal with several performance and power related problems such as application load balancing, VM consolidation, among others. AMs are categorized according to their overhead on the whole system. For instance, a VM consolidation may involve VM migration and thus should be sparsely performed, whereas the load balancing involves the startup/shutdown of a VM and therefore may be performed more frequently. The interference are managed by sharing the problems’ variables.

%Co-Con [14] is an approach for the coordination of multiple AMs in order to manage power and application-level performance in data centers. It consists of one cluter-level power controller, which adjusts the CPU frequencies of PMs; and several VM-level performance controllers, which adjust the resource fraction allocated the concerned VM based on its actual response time. To avoid interferences, the authors claim that a primary controller (power) must have a control period greater than the settling time of the secondary ones (performance), because hence the secondary ones can achieve a steady state within the control period of the primary one.

%On the one hand, in the context of cloud computing, it is required that managed systems (services) and AMs meet some constraints with respect to information hiding and loose-coupling. On the other hand, AMs which are completely unaware of others managed systems’ context may lead to decisions that impacts negatively services managed by other AMs. The work previously presented do not fit this context due to the high level of details shared among AMs such as application-level performance details or CPU frequencies levels. Instead, this work proposes an event-based coordination mechanism equipped with a public knowledge, which is used by AMs to share a piece of information that can be used within other AMs’ analyses process. The objective is to promote a synergy between AMs while keeping the required level of loose-coupling between AMs.